{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('crowd_final.csv')\n",
    "data = data.drop(data[data['relation']== 'IDK'].index).reset_index()\n",
    "data['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is anything missing\n",
    "print(data[data['text'].isnull()]['text'])\n",
    "print(data[data['relation'].isnull()]['relation'])\n",
    "print(data[data['DOID'].isnull()]['DOID'])\n",
    "print(data[data['DBID'].isnull()]['DBID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for cleaning \n",
    "\n",
    "def remove (x):\n",
    "    no_punct = \"\"\n",
    "    for char in x:\n",
    "            if char in '''qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM ''':\n",
    "                    no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "def remove_non_digits (x):\n",
    "    no_punct = \"\"\n",
    "    for char in x:\n",
    "            if char in '''1234567890qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM ''':\n",
    "                    no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "def lower (x):\n",
    "    return x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encode Labels ---\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['relation'])\n",
    "le.classes_\n",
    "labels_en = le.transform(data['relation']) \n",
    "\n",
    "# Check the encoding\n",
    "zero = list(le.inverse_transform([0]))\n",
    "one = list(le.inverse_transform([1]))\n",
    "two = list(le.inverse_transform([2]))\n",
    "three = list(le.inverse_transform([3]))\n",
    "\n",
    "print(zero, 'is encoded as 0')\n",
    "print(one, 'is encoded as 1')\n",
    "print(two, 'is encoded as 2')\n",
    "print(three, 'is encoded as 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encode Drugs ---\n",
    "\n",
    "le.fit(data['DBID'])\n",
    "le.classes_\n",
    "drug_labbeled = le.transform(data['DBID']) \n",
    "\n",
    "# --- Encode Disease ---\n",
    "\n",
    "le.fit(data['DOID'])\n",
    "le.classes_\n",
    "disease_labbeled = le.transform(data['DOID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['text'] = data['text'].apply(remove)\n",
    "data['text'] = data['text'].apply(lower)\n",
    "data['disease'] = data['disease'].apply(lower)\n",
    "data['drug'] = data['drug'].apply(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'label': labels_en, 'text': data['text'],'disease_name':data['disease'],  'disease': disease_labbeled,'drug_name':data['drug'], 'drug':drug_labbeled}\n",
    "data = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rule phrases\n",
    "\n",
    "phrases =  [\"hypersensitivity reactions\",\n",
    "\"associated with the risk of\",\n",
    "\"to the risk of\",\n",
    "\"a high risk for\",\n",
    "\"a high risk of\",\n",
    "\"high incidence of\", \n",
    "\"higher incidence of\", \n",
    "\" cause \",\n",
    "\" causes \",\n",
    "\"symptoms occure\",\n",
    "\"teratogenic\",\n",
    "\"site reaction\",\n",
    "\"the risk of development\",\n",
    "\"is associated with a risk of\",\n",
    "\"symptoms of the poisoning\",\n",
    "\"symptoms of poisoning\" ,\n",
    "\"not administrated to\",\n",
    "\"contraindicated in\",\n",
    "\"contraindicatedin\",\n",
    "\"should not be used\",\n",
    "\"is contraindication for\",\n",
    "\"is contraindication when\",\n",
    "\"is contraindicated when\",\n",
    "\"must not be used for\",\n",
    "\"do not administer\",\n",
    "\"should not initiate\",\n",
    "\"not be administered to\",\n",
    "\"do not initiate patients\",\n",
    "\"contraindication for\",\n",
    "\"should not be given\",\n",
    "\"do not use\",\n",
    "\"patients with a history of\",\n",
    "\"relief of the signs\",\n",
    "\"relief of the signs and symptoms of\",\n",
    "\"relief of signs\",\n",
    "\"relief of symptoms\",\n",
    "\"relief of the symptoms\",\n",
    "\"help\",\n",
    "\"helps\",\n",
    "\"relief of signs and symptoms of\",\n",
    "\"reduction of symptoms of\",\n",
    "\"treatment of the symptoms of\",\n",
    "\"for the relief\",\n",
    "\"management of the signs and symptoms of\", \n",
    "\" indicated for the treatment of\",\n",
    "\" indicated in the management of\",\n",
    "\" indicated for the management of\",\n",
    "\"for the management of\",\n",
    "\"management of\",\n",
    "\" indicated for the maintenance of remission\", \n",
    "\"or the treatment of\",\n",
    "\"in the treatment of\",\n",
    "\" indicated as\",\n",
    "\" indicated in\",\n",
    "\"be effective\",\n",
    "\"active treatment of\",\n",
    "\" indicated for\",\n",
    "\"treatment of\",\n",
    "\" indicated as an adjunct\",\n",
    "\" indicated for use in the treatment of\", \n",
    "\" indicated for the intermittent treatment\", \n",
    "\" indicated to reduce the rate of\",\n",
    "\" indicated for the rapid control\",\n",
    "\" indicated for the control\",\n",
    "\"reduce the risk of\",\n",
    "\" indicated as adjunctive treatment\",\n",
    "\"for the treatment of\",\n",
    "\" indicated as an adjunct\",\n",
    "\"areindicatedas\",\n",
    "\"treatment is indicated\",\n",
    "\"prophylaxis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features as distances of disease from each rule phrase\n",
    "def get_features (text, disease):\n",
    "    str = text\n",
    "    position_disease = str.find(disease)\n",
    "    \n",
    "    feature = []\n",
    "    for i in range(len(phrases)):\n",
    "        position_phrase = str.find(phrases[i])\n",
    "        if position_phrase != -1:\n",
    "            distance = abs(position_disease - position_phrase)\n",
    "            feature.append(distance)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def collect_features(df):\n",
    "    X_features =[]\n",
    "    for i in range(len(df)):\n",
    "        new_feature = get_features (df['text'][i], df['disease_name'][i])\n",
    "        X_features.append(new_feature)\n",
    "    return X_features\n",
    "\n",
    "X_features = collect_features(data)\n",
    "y = labels_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pycm\n",
    "from pycm import *\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10) #or n_splits=5\n",
    "\n",
    "X = X_features\n",
    "all_cm = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = [X[i] for i in train_index] , [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    \n",
    "    #class_weight = dict({0:4, 1:5, 2:4, 3:1})\n",
    "    #, class_weight = class_weight\n",
    "    model = svm.SVC(gamma= 'scale')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = ConfusionMatrix(y_pred, y_test)\n",
    "    \n",
    "    all_cm.append(cm)\n",
    "    \n",
    "confusion_matrix = pd.DataFrame()\n",
    "for i in range(len(all_cm)):\n",
    "      confusion_matrix= confusion_matrix.append(pd.DataFrame(all_cm[i].table))\n",
    "        \n",
    "confusion_matrix = confusion_matrix.groupby(confusion_matrix.index).sum()\n",
    "confusion_matrix.columns=['Contraindication', 'Effect', 'Relief', 'Treatment']\n",
    "confusion_matrix.index = ['Contraindication', 'Effect', 'Relief', 'Treatment']\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics from confusion matrix\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "PRE = (TP)/(TP+FP)\n",
    "REC = (TP)/(TP+FN)\n",
    "F1 = 2*(TP)/(2*TP+FP+FN)\n",
    "\n",
    "\n",
    "print('---Accuracy---')\n",
    "print(ACC)\n",
    "print('---Precision---')\n",
    "print(PRE)\n",
    "print('---Recall---')\n",
    "print(REC)\n",
    "print('---F1-score---')\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes  = FP.sum()\n",
    "mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pycm import *\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10) #or n_splits=5\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "X = X_features\n",
    "all_cm = []\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = [X[i] for i in train_index] , [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    \n",
    "    #class_weight = dict({0:4, 1:5, 2:4, 3:1})\n",
    "    #, class_weight = class_weight\n",
    "    \n",
    "    dt_model = DecisionTreeClassifier( criterion = 'entropy', random_state = 42)\n",
    "    dt_model = dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = dt_model.predict(X_test)\n",
    "    cm = ConfusionMatrix(y_test, y_pred)\n",
    "    \n",
    "    all_cm.append(cm)\n",
    "    \n",
    "# Combine the different confusion matrices from the k validation sets\n",
    "\n",
    "confusion_matrix = pd.DataFrame()\n",
    "for i in range(len(all_cm)):\n",
    "      confusion_matrix= confusion_matrix.append(pd.DataFrame(all_cm[i].table))\n",
    "        \n",
    "confusion_matrix = confusion_matrix.groupby(confusion_matrix.index).sum()\n",
    "confusion_matrix.columns=['Contraindication', 'Effect', 'Relief', 'Treatment']\n",
    "confusion_matrix.index = ['Contraindication', 'Effect', 'Relief', 'Treatment']\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics from confusion matrix\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "PRE = (TP)/(TP+FP)\n",
    "REC = (TP)/(TP+FN)\n",
    "F1 = 2*(TP)/(2*TP+FP+FN)\n",
    "\n",
    "print('---Accuracy---')\n",
    "print(ACC)\n",
    "print('---Precision---')\n",
    "print(PRE)\n",
    "print('---Recall---')\n",
    "print(REC)\n",
    "print('---F1-score---')\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes  = FP.sum()\n",
    "mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "X = X_features\n",
    "all_cm = []\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = [X[i] for i in train_index] , [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    #print (len(X_train), len(X_test))\n",
    "    \n",
    "    class_weight = dict({0:4, 1:5, 2:4, 3:1})\n",
    "    #, class_weight = class_weight\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "   \n",
    "    rf_model = rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    cm = ConfusionMatrix(y_test, y_pred)\n",
    "    \n",
    "    all_cm.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the different confusion matrices from the k validation sets\n",
    "\n",
    "confusion_matrix = pd.DataFrame()\n",
    "for i in range(len(all_cm)):\n",
    "      confusion_matrix= confusion_matrix.append(pd.DataFrame(all_cm[i].table))\n",
    "        \n",
    "confusion_matrix = confusion_matrix.groupby(confusion_matrix.index).sum()\n",
    "confusion_matrix.columns=['Contraindication', 'Effect', 'Relief', 'Treatment']\n",
    "confusion_matrix.index = ['Contraindication', 'Effect', 'Relief', 'Treatment']\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "PRE = (TP)/(TP+FP)\n",
    "REC = (TP)/(TP+FN)\n",
    "F1 = 2*(TP)/(2*TP+FP+FN)\n",
    "\n",
    "print('---Accuracy---')\n",
    "print(ACC)\n",
    "print('---Precision---')\n",
    "print(PRE)\n",
    "print('---Recall---')\n",
    "print(REC)\n",
    "print('---F1-score---')\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes  = FP.sum()\n",
    "mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
