{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snorkel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "crowd1 = pd.read_csv('crowd_round1_clean.csv')\n",
    "crowd1 = crowd1[['DB_ID', 'DO_ID', 'disease', 'Drug_name', 'Context', 'relation']]\n",
    "\n",
    "crowd2 = pd.read_csv('crowd_round2_clean.csv')\n",
    "crowd2 = crowd2[['DB_ID', 'DO_ID', 'disease', 'Drug_name', 'Context', 'relation']]\n",
    "\n",
    "\n",
    "crowd_df = crowd1\n",
    "\n",
    "df_test_1 = pd.read_csv('experts_curated_clean.csv')\n",
    "df_test_1 = df_test_1[['drug_id','do_id','disease_name', 'drug_name', 'context','Final']]\n",
    "df_test_1 = df_test_1.rename(columns={\"drug_id\": \"DB_ID\", \"do_id\": \"DO_ID\",\"disease_name\": \"disease\", \"drug_name\": \"Drug_name\", \"context\":\"Context\", \"Final\":\"relation\" })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = crowd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the test set in : test and validation set\n",
    "y = df_test_1['relation'] \n",
    "X = df_test_1.drop(['relation'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\n",
    "\n",
    "a,b = sss.split(X, y)\n",
    "\n",
    "train_index = a[0]\n",
    "dev_index = a[1]\n",
    "\n",
    "df_test = df_test_1.iloc[train_index]\n",
    "df_dev = df_test_1.iloc[dev_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contraindication'] is encoded as 0\n",
      "['effect'] is encoded as 1\n",
      "['relief'] is encoded as 2\n",
      "['treatment'] is encoded as 3\n"
     ]
    }
   ],
   "source": [
    "# Encode the classes\n",
    "\n",
    "Y_train = df_train.relation\n",
    "Y_test = df_test.relation\n",
    "Y_dev = df_dev.relation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y_train)\n",
    "le.classes_\n",
    "labels_en_train = le.transform(Y_train) \n",
    "labels_en_test = le.transform(Y_test) \n",
    "labels_en_dev = le.transform(Y_dev) \n",
    "# Check the encoding\n",
    "zero = list(le.inverse_transform([0]))\n",
    "one = list(le.inverse_transform([1]))\n",
    "two = list(le.inverse_transform([2]))\n",
    "three = list(le.inverse_transform([3]))\n",
    "\n",
    "print(zero, 'is encoded as 0')\n",
    "print(one, 'is encoded as 1')\n",
    "print(two, 'is encoded as 2')\n",
    "print(three, 'is encoded as 3')\n",
    "\n",
    "Y_train = labels_en_train\n",
    "Y_test = labels_en_test \n",
    "Y_dev = labels_en_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_CLASSES = {\n",
    "'contraindication': 0,\n",
    "'effect': 1,\n",
    "'relief': 2,\n",
    "'treatment':3 \n",
    "}\n",
    "\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phrases = { 'effect': \n",
    "[\"hypersensitivity reactions\",\n",
    "\"hypersensitivity reaction\",\n",
    "\"associated with the risk of\",\n",
    "\"to the risk of\",\n",
    "\"a high risk for\",\n",
    "\"a high risk of\",\n",
    "\"high incidence of\", \n",
    "\"higher incidence of\", \n",
    "\" cause \",\n",
    "\" causes \",\n",
    "\"symptoms occure\",\n",
    "\"teratogenic\",\n",
    "\"site reaction\",\n",
    "\"the risk of development\",\n",
    "\"is associated with a risk of\",\n",
    "\"symptoms of the poisoning\",\n",
    "\"symptoms of poisoning\",\n",
    "\"is the result of\",\n",
    "\"might prove harmful\"\n",
    "],\n",
    "\n",
    " 'contraindication': \n",
    "[\"not administrated to\",\n",
    "\"is contraindicated\", \n",
    "\"contraindicated in\",\n",
    "\"contraindicatedin\",\n",
    "\"contraindicated with\",\n",
    "\"should not be used\",\n",
    "\"is contraindication for\",\n",
    "\"is contraindication when\",\n",
    "\"is contraindicated when\",\n",
    "\"is contraindicated for\",\n",
    "\"must not be used for\",\n",
    "\"do not administer\",\n",
    "\"should not initiate\",\n",
    "\"not be administered to\",\n",
    "\"do not initiate patients\",\n",
    "\"contraindication for\",\n",
    "\"should not be given\",\n",
    "\"do not use\",\n",
    "\"patients with a history of\",\n",
    "\"history of\", \n",
    "\"adverse reactions\",\n",
    "\"that have the following conditions\",\n",
    "\"are not candidates for\", \n",
    "\"contraindication\",\n",
    "\"allergy or hypersensitivity\",\n",
    "\"are contraindicated\",\n",
    "\"should not be employed\",\n",
    "\"contraindicatedin\"],\n",
    "              \n",
    " 'relief': \n",
    "[\"relief of the signs\",\n",
    "\"relief of the signs and symptoms of\",\n",
    "\"relief of signs\",\n",
    "\"relief of symptoms\",\n",
    "\"relief of the symptoms\",\n",
    "\"help\",\n",
    "\"helps\",\n",
    "\"relief of signs and symptoms of\",\n",
    "\"reduction of symptoms of\",\n",
    "\"treatment of the symptoms of\",\n",
    "\"for the relief\",\n",
    "\"management of the signs and symptoms of\"], \n",
    "\n",
    " 'treatment':\n",
    "[\" indicated for the treatment of\",\n",
    "\"reduce the development of\",\n",
    "\" indicated for the management of\",\n",
    "\"for the management of\",\n",
    "\"management of\",\n",
    "\" indicated for the maintenance of remission\", \n",
    "\"or the treatment of\",\n",
    "\"in the treatment of\",\n",
    "\" indicated as\",\n",
    "\" indicated in\",\n",
    "\"be effective\",\n",
    "\"active treatment of\",\n",
    "\" indicated for\",\n",
    "\"treatment of\",\n",
    "\" indicated as an adjunct\",\n",
    "\" indicated for use in the treatment of\", \n",
    "\" indicated for the intermittent treatment\", \n",
    "\" indicated to reduce the rate of\",\n",
    "\" indicated for the rapid control\",\n",
    "\" indicated for the control\",\n",
    "\"reduce the risk of\",\n",
    "\" indicated as adjunctive treatment\",\n",
    "\"for the treatment of\",\n",
    "\" indicated as an adjunct\",\n",
    "\"areindicatedas\",\n",
    "\"treatment is indicated\",\n",
    "\"is indicatedin\",\n",
    "\"indicated to improve\",\n",
    "\"healing of \",\n",
    "\"supplemental therapy\"]\n",
    "           }\n",
    "\n",
    "phrases = pd.DataFrame({ key:pd.Series(value) for key, value in my_phrases.items() })\n",
    "\n",
    "effect = { 'phrases': \n",
    "[\"hypersensitivity reactions\",\n",
    "\"hypersensitivity reaction\",\n",
    "\"associated with the risk of\",\n",
    "\"to the risk of\",\n",
    "\"a high risk for\",\n",
    "\"a high risk of\",\n",
    "\"high incidence of\", \n",
    "\"higher incidence of\", \n",
    "\" cause \",\n",
    "\" causes \",\n",
    "\"symptoms occure\",\n",
    "\"teratogenic\",\n",
    "\"site reaction\",\n",
    "\"the risk of development\",\n",
    "\"is associated with a risk of\",\n",
    "\"symptoms of the poisoning\",\n",
    "\"symptoms of poisoning\",\n",
    "\"is the result of\",\n",
    "\"might prove harmful\"\n",
    "]}\n",
    "effect_df = pd.DataFrame(effect)\n",
    "\n",
    " \n",
    "contraindication = {'phrases': \n",
    "[\"not administrated to\",\n",
    "\"is contraindicated\", \n",
    "\"contraindicated in\",\n",
    "\"contraindicatedin\",\n",
    "\"contraindicated with\",\n",
    "\"should not be used\",\n",
    "\"is contraindication for\",\n",
    "\"is contraindication when\",\n",
    "\"is contraindicated when\",\n",
    "\"is contraindicated for\",\n",
    "\"must not be used for\",\n",
    "\"do not administer\",\n",
    "\"should not initiate\",\n",
    "\"not be administered to\",\n",
    "\"do not initiate patients\",\n",
    "\"contraindication for\",\n",
    "\"should not be given\",\n",
    "\"do not use\",\n",
    "\"patients with a history of\",\n",
    "\"history of\", \n",
    "\"adverse reactions\",\n",
    "\"that have the following conditions\",\n",
    "\"are not candidates for\", \n",
    "\"contraindication\",\n",
    "\"allergy or hypersensitivity\",\n",
    "\"are contraindicated\",\n",
    "\"should not be employed\",\n",
    "\"contraindicatedin\"]}\n",
    "contraindication_df = pd.DataFrame(contraindication)\n",
    "              \n",
    "relief = {'phrases': \n",
    "[\"relief of the signs\",\n",
    "\"relief of the signs and symptoms of\",\n",
    "\"relief of signs\",\n",
    "\"relief of symptoms\",\n",
    "\"relief of the symptoms\",\n",
    "\"help\",\n",
    "\"helps\",\n",
    "\"relief of signs and symptoms of\",\n",
    "\"reduction of symptoms of\",\n",
    "\"treatment of the symptoms of\",\n",
    "\"for the relief\",\n",
    "\"management of the signs and symptoms of\"]} \n",
    "relief_df = pd.DataFrame(relief)\n",
    "\n",
    "treatment = {'phrases':\n",
    "[\" indicated for the treatment of\",\n",
    "\"reduce the development of\",\n",
    "\" indicated for the management of\",\n",
    "\"for the management of\",\n",
    "\"management of\",\n",
    "\" indicated for the maintenance of remission\", \n",
    "\"or the treatment of\",\n",
    "\"in the treatment of\",\n",
    "\" indicated as\",\n",
    "\" indicated in\",\n",
    "\"be effective\",\n",
    "\"active treatment of\",\n",
    "\" indicated for\",\n",
    "\"treatment of\",\n",
    "\" indicated as an adjunct\",\n",
    "\" indicated for use in the treatment of\", \n",
    "\" indicated for the intermittent treatment\", \n",
    "\" indicated to reduce the rate of\",\n",
    "\" indicated for the rapid control\",\n",
    "\" indicated for the control\",\n",
    "\"reduce the risk of\",\n",
    "\" indicated as adjunctive treatment\",\n",
    "\"for the treatment of\",\n",
    "\" indicated as an adjunct\",\n",
    "\"areindicatedas\",\n",
    "\"treatment is indicated\",\n",
    "\"is indicatedin\",\n",
    "\"indicated to improve\",\n",
    "\"healing of \",\n",
    "\"supplemental therapy\"]}\n",
    "treatment_df = pd.DataFrame(treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x, string):\n",
    "    return x in string\n",
    "\n",
    "\n",
    "def find_phrases(text):\n",
    "    # Find which phrases appear in the sentence\n",
    "    class_effect = effect_df['phrases'].apply(lambda num : check(num,text))\n",
    "    class_contraindication = contraindication_df['phrases'].apply(lambda num : check(num,text))\n",
    "    class_relief = relief_df['phrases'].apply(lambda num : check(num,text))\n",
    "    class_treatment = treatment_df['phrases'].apply(lambda num : check(num,text))\n",
    "    \n",
    "    #Create a dataset from them\n",
    "    classes = {\n",
    "    'effect': class_effect,\n",
    "    'contraindication': class_contraindication,\n",
    "    'relief': class_relief,\n",
    "    'treatment':class_treatment \n",
    "    }\n",
    "\n",
    "    classes = pd.DataFrame(classes)\n",
    "    \n",
    "    a = class_effect.index[class_effect == True].tolist()\n",
    "    b = class_contraindication.index[class_contraindication == True].tolist()\n",
    "    c = class_relief.index[class_relief == True].tolist()\n",
    "    d = class_treatment.index[class_treatment == True].tolist()\n",
    "    return classes, a, b, c, d\n",
    "\n",
    "# return the position of a phrase/word in a sentence\n",
    "def pos_of_phrase(string, substring):\n",
    "    substring_length = len(substring)    \n",
    "    def recurse(locations_found, start):\n",
    "        location = string.find(substring, start)\n",
    "        if location != -1:\n",
    "            return recurse(locations_found + [location], location+substring_length)\n",
    "        else:\n",
    "            return locations_found\n",
    "\n",
    "    return recurse([], 0)\n",
    "\n",
    "# return the minimum distance of all phrase/word from the target disease\n",
    "def get_diff_of_pos(classes, column, pos, string):\n",
    "    name = classes.columns[column]\n",
    "    my_col = classes[name]\n",
    "    x = [k for k,v in enumerate(my_col) if v == True]\n",
    "    \n",
    "    candidate = []\n",
    "    distances = []\n",
    "    for i in range(len(x)):\n",
    "        word = phrases[classes.columns[column]][x[i]]\n",
    "        \n",
    "        for i in range(len(pos_of_phrase(string, word))):\n",
    "            distances.append( abs(pos -   pos_of_phrase(string, word)[i]) )\n",
    "\n",
    "    return min(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the labelling functions\n",
    "\n",
    "@labeling_function()\n",
    "def phrases_claas(x):\n",
    "    \n",
    "    text = x.Context\n",
    "\n",
    "    classes, a, b, c, d =find_phrases(text)\n",
    "\n",
    "\n",
    "    # Check if you have unique class and label based on that \n",
    "    if len(a)!=0 and len(b)==len(c)==len(d)==0:\n",
    "        #print('effect')\n",
    "        return RELATION_CLASSES['effect']\n",
    "    elif len(b)!=0 and len(a)==len(c)==len(d)==0:\n",
    "        #print('contraindication')\n",
    "        return RELATION_CLASSES['contraindication']\n",
    "    elif len(c)!=0 and len(a)==len(b)==len(d)==0:\n",
    "        #print('relief')\n",
    "        return  RELATION_CLASSES['relief']\n",
    "    elif len(d)!=0 and len(a)==len(b)==len(c)==0:\n",
    "        #print('treatment')\n",
    "        return  RELATION_CLASSES['treatment']\n",
    "    elif len(a)==len(b)==len(c)==len(d)==0:\n",
    "        #print('IDK')\n",
    "        return ABSTAIN\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def phrase_closeness(x):\n",
    "    text = x.Context\n",
    "    pos1 = text.find(x.disease)\n",
    "    dis_len = len(x.disease)\n",
    "    classes, a, b, c, d = find_phrases(text)\n",
    "    \n",
    "    minDist = len(text.split())\n",
    "    category = ABSTAIN\n",
    "    for class_,found_phrases in {'effect':a, 'contraindication':b, 'relief':c, 'treatment':d}.items():\n",
    "        for phrase_index in found_phrases:\n",
    "            phrase =phrases.loc[phrase_index,class_]\n",
    "            pos2 = text.find(phrase)\n",
    "            phr_len = len(phrase)\n",
    "            if pos1 > pos2:\n",
    "                span = text[pos2+phr_len:pos1]\n",
    "            else:\n",
    "                span = text[pos1+dis_len:pos2]\n",
    "            words = span.split()\n",
    "            distance = len(words)\n",
    "            if minDist > distance:\n",
    "                minDist = distance\n",
    "                category = class_\n",
    "    \n",
    "    if category == ABSTAIN:\n",
    "        return ABSTAIN\n",
    "    return RELATION_CLASSES[category]\n",
    "\n",
    "@labeling_function()\n",
    "def crowd(x):\n",
    "    #print (x.relation_type)\n",
    "    if x.relation in RELATION_CLASSES:\n",
    "        return RELATION_CLASSES[x.relation]\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8874c30b5e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test.relation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [crowd, phrase_closeness, phrases_claas]\n",
    "applier = PandasLFApplier(lfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2018 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 18/2018 [00:00<00:11, 174.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 36/2018 [00:00<00:11, 175.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 56/2018 [00:00<00:10, 179.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 76/2018 [00:00<00:10, 183.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 96/2018 [00:00<00:10, 187.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 116/2018 [00:00<00:10, 189.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 136/2018 [00:00<00:09, 190.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 156/2018 [00:00<00:09, 191.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 176/2018 [00:00<00:09, 192.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 196/2018 [00:01<00:09, 192.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 216/2018 [00:01<00:09, 194.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 236/2018 [00:01<00:09, 193.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 256/2018 [00:01<00:09, 193.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▎        | 276/2018 [00:01<00:08, 194.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 296/2018 [00:01<00:08, 191.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 316/2018 [00:01<00:08, 192.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 336/2018 [00:01<00:08, 192.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 356/2018 [00:01<00:08, 192.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▊        | 376/2018 [00:01<00:08, 193.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|█▉        | 396/2018 [00:02<00:08, 193.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 416/2018 [00:02<00:08, 194.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 436/2018 [00:02<00:08, 194.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 456/2018 [00:02<00:08, 194.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▎       | 476/2018 [00:02<00:07, 194.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▍       | 496/2018 [00:02<00:07, 194.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 516/2018 [00:02<00:07, 195.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 536/2018 [00:02<00:07, 193.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 556/2018 [00:02<00:07, 193.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 576/2018 [00:02<00:07, 194.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 596/2018 [00:03<00:07, 193.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 616/2018 [00:03<00:07, 193.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 636/2018 [00:03<00:07, 193.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 656/2018 [00:03<00:07, 194.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 676/2018 [00:03<00:06, 194.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 696/2018 [00:03<00:06, 192.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 716/2018 [00:03<00:06, 191.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 736/2018 [00:03<00:06, 191.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 756/2018 [00:03<00:06, 193.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 776/2018 [00:04<00:06, 191.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 796/2018 [00:04<00:06, 192.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 816/2018 [00:04<00:06, 192.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 836/2018 [00:04<00:06, 191.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 856/2018 [00:04<00:06, 190.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 876/2018 [00:04<00:05, 192.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 896/2018 [00:04<00:05, 192.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 916/2018 [00:04<00:05, 192.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▋     | 936/2018 [00:04<00:05, 192.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 956/2018 [00:04<00:05, 192.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 976/2018 [00:05<00:05, 193.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 996/2018 [00:05<00:05, 194.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 1016/2018 [00:05<00:05, 193.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████▏    | 1036/2018 [00:05<00:05, 193.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 1056/2018 [00:05<00:05, 192.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 1076/2018 [00:05<00:04, 192.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 1096/2018 [00:05<00:04, 193.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 1116/2018 [00:05<00:04, 193.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 1136/2018 [00:05<00:04, 194.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 1156/2018 [00:06<00:04, 192.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 1176/2018 [00:06<00:04, 192.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 1196/2018 [00:06<00:04, 192.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 1216/2018 [00:06<00:04, 191.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 1236/2018 [00:06<00:04, 192.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 1256/2018 [00:06<00:03, 192.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 1276/2018 [00:06<00:03, 192.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▍   | 1296/2018 [00:06<00:03, 191.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 1316/2018 [00:06<00:03, 191.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 1336/2018 [00:06<00:03, 191.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 1356/2018 [00:07<00:03, 191.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 1376/2018 [00:07<00:03, 192.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 1396/2018 [00:07<00:03, 193.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 1416/2018 [00:07<00:03, 193.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 1436/2018 [00:07<00:03, 193.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 1456/2018 [00:07<00:02, 193.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 1476/2018 [00:07<00:02, 193.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 1496/2018 [00:07<00:02, 193.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 1516/2018 [00:07<00:02, 193.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 1536/2018 [00:07<00:02, 192.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 1556/2018 [00:08<00:02, 193.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 1576/2018 [00:08<00:02, 194.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 1596/2018 [00:08<00:02, 194.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 1616/2018 [00:08<00:02, 194.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 1636/2018 [00:08<00:01, 193.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 1656/2018 [00:08<00:01, 194.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 1676/2018 [00:08<00:01, 194.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 1696/2018 [00:08<00:01, 194.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 1716/2018 [00:08<00:01, 193.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 1736/2018 [00:09<00:01, 193.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 1756/2018 [00:09<00:01, 195.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 1776/2018 [00:09<00:01, 194.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 1796/2018 [00:09<00:01, 192.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 1816/2018 [00:09<00:01, 191.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 1836/2018 [00:09<00:00, 191.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 1856/2018 [00:09<00:00, 192.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 1876/2018 [00:09<00:00, 192.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 1896/2018 [00:09<00:00, 186.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▍| 1916/2018 [00:09<00:00, 188.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 1936/2018 [00:10<00:00, 190.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 1956/2018 [00:10<00:00, 192.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 1976/2018 [00:10<00:00, 192.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 1996/2018 [00:10<00:00, 190.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2018/2018 [00:10<00:00, 192.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "L_train = applier.apply(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/358 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 17/358 [00:00<00:02, 166.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 35/358 [00:00<00:01, 168.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 54/358 [00:00<00:01, 174.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 74/358 [00:00<00:01, 178.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 94/358 [00:00<00:01, 183.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 115/358 [00:00<00:01, 188.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 135/358 [00:00<00:01, 189.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 155/358 [00:00<00:01, 191.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 175/358 [00:00<00:00, 191.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 194/358 [00:01<00:00, 188.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 213/358 [00:01<00:00, 185.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▍   | 232/358 [00:01<00:00, 183.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 251/358 [00:01<00:00, 184.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 271/358 [00:01<00:00, 186.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 290/358 [00:01<00:00, 187.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▋ | 309/358 [00:01<00:00, 186.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 328/358 [00:01<00:00, 184.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 358/358 [00:01<00:00, 186.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "L_test= applier.apply(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "# Train LabelModel.\n",
    "label_model = LabelModel(cardinality=4, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=200, seed=123, log_freq=20, l2=0.1, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "\n",
    "preds_dev = label_model.predict(L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 3, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 3,\n",
       "       1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [179, 358]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-898-0d83ad7cf97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/snorkel/analysis/metrics.py\u001b[0m in \u001b[0;36mmetric_score\u001b[0;34m(golds, preds, probs, metric, filter_dict, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mlabel_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabel_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [179, 358]"
     ]
    }
   ],
   "source": [
    "acc = metric_score(Y_test, preds_dev, probs=None, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8603351955307262"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crowd</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983240</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase_closeness</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>0.983240</td>\n",
       "      <td>0.983240</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>132</td>\n",
       "      <td>44</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrases_claas</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>0.553073</td>\n",
       "      <td>0.553073</td>\n",
       "      <td>0.089385</td>\n",
       "      <td>83</td>\n",
       "      <td>16</td>\n",
       "      <td>0.838384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  j      Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "crowd             0  [0, 1, 2, 3]  1.000000  0.983240   0.245810      179   \n",
       "phrase_closeness  1  [0, 1, 2, 3]  0.983240  0.983240   0.245810      132   \n",
       "phrases_claas     2  [0, 1, 2, 3]  0.553073  0.553073   0.089385       83   \n",
       "\n",
       "                  Incorrect  Emp. Acc.  \n",
       "crowd                     0   1.000000  \n",
       "phrase_closeness         44   0.750000  \n",
       "phrases_claas            16   0.838384  "
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
